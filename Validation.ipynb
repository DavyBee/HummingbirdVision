{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e788ae",
   "metadata": {},
   "source": [
    "This notebook follows the Ultralytics docs outline here: [Ultralytics Validation](https://docs.ultralytics.com/modes/val/). \n",
    "\n",
    "With this notebook, you can verify the statistics of your model. The output metrics will not be exact, as the pipeline here contains some post processing on the model outputs that Ultralytics does not do in the background of this validation. However, it should stand as a fairly good guide.\n",
    "\n",
    "Run the code below to get an indication of the model precision, recall, and Mean-Average-Precision (MAP) across different confidence levels. \n",
    "\n",
    "Important to note, the precision listed by the validate function is not totally indicitive of the precision of the model on hummingbirds. Rather, it combines the precision for hummingbirds with the precision for detecting a background. Usually, the bird precision is lower than the background precision, but you can manually calculate the precision by looking at the confusion matrix generated by the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33924e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = 'model_weights/YOLO.pt'\n",
    "data_path = 'dataset/data.yaml'\n",
    "\n",
    "model = YOLO(model_path)\n",
    "\n",
    "model.val(data = data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89192a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import RTDETR\n",
    "## similar code below"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
